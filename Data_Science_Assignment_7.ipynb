{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJ5BWcnr8mDu"
      },
      "outputs": [],
      "source": [
        "! pip install -q tensorflow_datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "XKF8qJ6d82cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphs(history, metric):\n",
        "    plt.plot(history.history[metric])\n",
        "    plt.plot(history.history['val_'+metric], '')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.legend([metric, 'val_'+metric])"
      ],
      "metadata": {
        "id": "3ki5P1vf831a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
        "                          as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "train_dataset.element_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctvMSpgW84EC",
        "outputId": "db620042-8ae9-42c8-e329-c2dee1c11677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\n",
            "Dataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "    print(f'Text: {example.numpy()} \\n')\n",
        "    print(f'Label: {label.numpy()} \\n')\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pb6YttHO84Oa",
        "outputId": "e7f6c091-b735-4fc5-e3b8-8236f8202688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\" \n",
            "\n",
            "Label: 0 \n",
            "\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "J1xJ0g1u84Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "    print(f\"Texts : {example.numpy()[:3]} \\n\")\n",
        "    print(f\"Labels: {label.numpy()[:3]} \\n\")\n",
        "    print(\"----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHCtG4Fh84ki",
        "outputId": "ec2b0239-76a1-49f9-f869-f34d17557882"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texts : [b\"They made me watch this in school and it was terrible. The movie is outdated. The episodes become confusing because fact is combined with fiction to make the story more interesting.The teachers talked about it as a treat but really it was a painfully boring experience.I have read that very few people who appear in this are actors, but most of them them do what they do in the movie in real life.This accounts for cheesy acting very often. Also, very often the story becomes mildly outrageous and far-fetched. I don't like the way some of the lines were written and wish they had more meaning to them. Though, it was written to be educational, funny, suspenseful, and hip, It ended up being boring, dry, far-fetched, and old. I hope no one takes time to watch this movie because you would be just fine not seeing it.\"\n",
            " b'Every generation fully believes it is living in the end times. This has been true for thousands of years now. And movies like this feed on this. How did they get the great Orson Welles to narrate this train wreck? This is a documentary about the biblical prophecies of Armageddon. It tries to link the prophecies as well as it can to what was happening in the times it was made, making it obviously dated and kind of silly.<br /><br />The reenactments look like they are out of \"Unsolved Mysteries\" but without the high production values. People should have been embarrassed to take part in this.<br /><br />In short, the movie is dated, silly, reactionary, and useless. Good if you want a good laugh, but not good enough to actually look for.'\n",
            " b'I just saw this movie today with my children (son, 10 and daughter, 4.5) at the 3rd Annual Roger Ebert Overlooked Film Festival. After the film the children in the audience were allowed to ask questions to the Director, Tian-Ming Wu. He (through a translator) told several stories about his life and the making of the film.<br /><br />All tangents aside, both of my children really enjoyed this movie. Of course, I had to paraphrase many of the subtitles for my daughter, but much of the film is visually self-explanatory.<br /><br />I won\\'t give anything away, but the bottom line is that this film is SO MUCH better than 95% of the Hollywood crap (especially children\\'s films) out there.<br /><br />Cheers.<br /><br />p.s. There is a \"real\"/original King of Masks who can/could do 12 masks at once. The actor in the movie trained and learned to do up to 4 masks at a time (then they would cut and change to 4 new masks).'] \n",
            "\n",
            "Labels: [0 0 1] \n",
            "\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 1000\n",
        "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(train_dataset.map(lambda text, label: text))\n",
        "\n",
        "vocab = np.array(encoder.get_vocabulary())"
      ],
      "metadata": {
        "id": "KxDVS_fv84uK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xU-flS-u844S",
        "outputId": "1bd36ccc-bed9-47dd-a24f-c1487bc09413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n",
              "       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n",
              "      dtype='<U14')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_example = encoder(example)[:3].numpy()\n",
        "encoded_example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oip1eVwr85Ky",
        "outputId": "b13893dc-0f3e-4d75-fb8f-8218d1c014d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 35,  91,  70, ...,   0,   0,   0],\n",
              "       [168,   1,   1, ...,   0,   0,   0],\n",
              "       [ 10,  41, 208, ...,   0,   0,   0]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n in range(3):\n",
        "  print(\"Original: \", example[n].numpy())\n",
        "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ocal3o885Va",
        "outputId": "3c36f8e7-1e70-4507-c58e-b8d30e98d2fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  b\"They made me watch this in school and it was terrible. The movie is outdated. The episodes become confusing because fact is combined with fiction to make the story more interesting.The teachers talked about it as a treat but really it was a painfully boring experience.I have read that very few people who appear in this are actors, but most of them them do what they do in the movie in real life.This accounts for cheesy acting very often. Also, very often the story becomes mildly outrageous and far-fetched. I don't like the way some of the lines were written and wish they had more meaning to them. Though, it was written to be educational, funny, suspenseful, and hip, It ended up being boring, dry, far-fetched, and old. I hope no one takes time to watch this movie because you would be just fine not seeing it.\"\n",
            "Round-trip:  they made me watch this in school and it was terrible the movie is [UNK] the episodes become [UNK] because fact is [UNK] with [UNK] to make the story more [UNK] [UNK] [UNK] about it as a [UNK] but really it was a [UNK] boring [UNK] have read that very few people who appear in this are actors but most of them them do what they do in the movie in real [UNK] [UNK] for cheesy acting very often also very often the story becomes [UNK] [UNK] and [UNK] i dont like the way some of the lines were written and wish they had more [UNK] to them though it was written to be [UNK] funny [UNK] and [UNK] it [UNK] up being boring [UNK] [UNK] and old i hope no one takes time to watch this movie because you would be just fine not seeing it                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
            "\n",
            "Original:  b'Every generation fully believes it is living in the end times. This has been true for thousands of years now. And movies like this feed on this. How did they get the great Orson Welles to narrate this train wreck? This is a documentary about the biblical prophecies of Armageddon. It tries to link the prophecies as well as it can to what was happening in the times it was made, making it obviously dated and kind of silly.<br /><br />The reenactments look like they are out of \"Unsolved Mysteries\" but without the high production values. People should have been embarrassed to take part in this.<br /><br />In short, the movie is dated, silly, reactionary, and useless. Good if you want a good laugh, but not good enough to actually look for.'\n",
            "Round-trip:  every [UNK] [UNK] [UNK] it is living in the end times this has been true for [UNK] of years now and movies like this [UNK] on this how did they get the great [UNK] [UNK] to [UNK] this [UNK] [UNK] this is a documentary about the [UNK] [UNK] of [UNK] it tries to [UNK] the [UNK] as well as it can to what was [UNK] in the times it was made making it obviously [UNK] and kind of [UNK] br the [UNK] look like they are out of [UNK] [UNK] but without the high production [UNK] people should have been [UNK] to take part in [UNK] br in short the movie is [UNK] silly [UNK] and [UNK] good if you want a good laugh but not good enough to actually look for                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
            "\n",
            "Original:  b'I just saw this movie today with my children (son, 10 and daughter, 4.5) at the 3rd Annual Roger Ebert Overlooked Film Festival. After the film the children in the audience were allowed to ask questions to the Director, Tian-Ming Wu. He (through a translator) told several stories about his life and the making of the film.<br /><br />All tangents aside, both of my children really enjoyed this movie. Of course, I had to paraphrase many of the subtitles for my daughter, but much of the film is visually self-explanatory.<br /><br />I won\\'t give anything away, but the bottom line is that this film is SO MUCH better than 95% of the Hollywood crap (especially children\\'s films) out there.<br /><br />Cheers.<br /><br />p.s. There is a \"real\"/original King of Masks who can/could do 12 masks at once. The actor in the movie trained and learned to do up to 4 masks at a time (then they would cut and change to 4 new masks).'\n",
            "Round-trip:  i just saw this movie today with my children son 10 and daughter [UNK] at the [UNK] [UNK] [UNK] [UNK] [UNK] film [UNK] after the film the children in the audience were [UNK] to ask [UNK] to the director [UNK] [UNK] he through a [UNK] told several stories about his life and the making of the filmbr br all [UNK] [UNK] both of my children really enjoyed this movie of course i had to [UNK] many of the [UNK] for my daughter but much of the film is [UNK] [UNK] br i wont give anything away but the [UNK] line is that this film is so much better than [UNK] of the hollywood crap especially [UNK] films out [UNK] br [UNK] br [UNK] there is a [UNK] king of [UNK] who [UNK] do [UNK] [UNK] at once the actor in the movie [UNK] and [UNK] to do up to 4 [UNK] at a time then they would cut and change to 4 new [UNK]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=64,\n",
        "        # Use masking to handle the variable sequence lengths\n",
        "        mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "JOut15Ci85fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "bHEktJNo-ld-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_dataset, epochs=10,\n",
        "                    validation_data=test_dataset,\n",
        "                    validation_steps=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6r9jkBI_w1k",
        "outputId": "c484aba9-0d7d-4142-b70b-d0a4394f4bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 1377s 3s/step - loss: 0.6097 - accuracy: 0.6143 - val_loss: 0.4249 - val_accuracy: 0.8234\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 1321s 3s/step - loss: 0.3861 - accuracy: 0.8301 - val_loss: 0.3407 - val_accuracy: 0.8521\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 1323s 3s/step - loss: 0.3404 - accuracy: 0.8528 - val_loss: 0.3488 - val_accuracy: 0.8224\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 1303s 3s/step - loss: 0.3236 - accuracy: 0.8614 - val_loss: 0.3355 - val_accuracy: 0.8427\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 1321s 3s/step - loss: 0.3127 - accuracy: 0.8655 - val_loss: 0.3123 - val_accuracy: 0.8740\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 1341s 3s/step - loss: 0.3067 - accuracy: 0.8680 - val_loss: 0.3244 - val_accuracy: 0.8630\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 1283s 3s/step - loss: 0.3035 - accuracy: 0.8698 - val_loss: 0.3416 - val_accuracy: 0.8594\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 1282s 3s/step - loss: 0.3012 - accuracy: 0.8708 - val_loss: 0.3197 - val_accuracy: 0.8448\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 1341s 3s/step - loss: 0.2976 - accuracy: 0.8703 - val_loss: 0.2940 - val_accuracy: 0.8719\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 1394s 4s/step - loss: 0.2961 - accuracy: 0.8721 - val_loss: 0.3467 - val_accuracy: 0.8526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNKHDdkh-myF",
        "outputId": "56b2919f-75e3-4071-b0cf-347e052d91e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 8s 8s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.00228294]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "HxDRy8gq-nFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Z0BxDgFx0fRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_dataset, epochs=10,\n",
        "                    validation_data=test_dataset,\n",
        "                    validation_steps=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yn5yLjkB-nVl",
        "outputId": "cec70386-486b-41b5-98b7-6e86fe5f1be1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 1298s 3s/step - loss: 0.2929 - accuracy: 0.8746 - val_loss: 0.3075 - val_accuracy: 0.8542\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 1329s 3s/step - loss: 0.2911 - accuracy: 0.8727 - val_loss: 0.3043 - val_accuracy: 0.8687\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 1255s 3s/step - loss: 0.2882 - accuracy: 0.8753 - val_loss: 0.3344 - val_accuracy: 0.8469\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 1267s 3s/step - loss: 0.2866 - accuracy: 0.8771 - val_loss: 0.3279 - val_accuracy: 0.8594\n",
            "Epoch 5/10\n",
            "385/391 [============================>.] - ETA: 18s - loss: 0.2847 - accuracy: 0.8771"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eDB4-RFi-nj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gM5Wiipy-nxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Ma9X0di-n-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iogs-VXp-oLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IbnZkP5f-oYe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}