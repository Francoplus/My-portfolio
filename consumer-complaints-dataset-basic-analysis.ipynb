{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consumer Complaints Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data__: Consumer complaints received about financial products and services<br>\n",
    "\n",
    "These are real world complaints received about financial products and services. Each complaint has been labeled with a specific product; therefore, this is a supervised text classification problem. With the aim to classify future complaints based on its content, we used different machine learning algorithms can make more accurate predictions (i.e., classify the complaint in one of the product categories)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "\n",
    "* [Goal](#obj)\n",
    "* [Importing packages and loading data](#imp)\n",
    "* [Exploratory Data Analysis (EDA) and Feature Engineering](#eda)\n",
    "* [Text Preprocessing](#pre)\n",
    "* [Multi-Classification models](#ml)\n",
    "    * [Spliting the data: train and test](#sp)\n",
    "    * [Models](#m)\n",
    "* [Comparison of model performance](#sum)\n",
    "* [Model Evaluation](#ev)\n",
    "    * [Precision, Recall, F1-score](#f1)\n",
    "    * [Confusion Matrix](#cm)\n",
    "* [Predictions](#pred)\n",
    "* [Classification Models and Feature Engineering](#pred)\n",
    "* [Exploratory Data Analysis (EDA) and Feature Engineering](#pred)\n",
    "* [Model Building](#ml)\n",
    "    * [Logistic Regression](#sp)\n",
    "    * [Random Forest Classifier](#ml)\n",
    "        * [GridSearchCV](#sp)\n",
    "        * [XGBoost](#ml)\n",
    "        * [XGBoost with GridSearchCV](#ml)\n",
    "        * [LightGBM](#ml)\n",
    "* [Predictions](#pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='obj'></a>\n",
    "## Goal:<br>\n",
    "Classification of Consumer Narratives to their respective Product Category and Predicting if a consumer will dispute or not based on the features.<br><br>\n",
    "Classification algorithms: Linear Support Vector Machine (LinearSVM), Random Forest, Multinomial Naive Bayes and Logistic Regression.<br><br>\n",
    "Note: Text classification is an example of supervised machine learning since we train the model with labelled data (complaints about and specific finance product is used for train a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='imp'></a>\n",
    "## Importing packages and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import randint\n",
    "import seaborn as sns # used for plot interactive graph. \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import cross_val_score,GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.tools as tls\n",
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "df = pd.read_csv('../input/consumer-complaints-train-dataset/Edureka_Consumer_Complaints_train.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3,58,810 (rows) and 18 features (columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2).T # Columns are shown in rows for easy reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,12))\n",
    "df.groupby(['State'])['Complaint ID'].count().sort_values().plot.barh(\n",
    "    ylim=0, color='blue', title= 'NUMBER OF COMPLAINTS IN EACH PRODUCT CATEGORY\\n')\n",
    "plt.xlabel('Number of ocurrences', fontsize = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "California has the highest number of complaints as compared to others. Let's see what are these complaints about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['State'] == 'CA']['Product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['State'] == 'CA']['Product'].value_counts().head(5).plot.pie(explode=[0.2,0,0,0,0],shadow=True)\n",
    "# Unsquish the pie.\n",
    "import matplotlib.pyplot as plt\n",
    "plt.gca().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state of California mainly has most complaints around Mortgage. Let's find out what kind of issues are raised for this particular product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['State'] == 'CA']['Issue'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['State'] == 'CA']['Issue'].value_counts().head(5).plot.pie(explode=[0.2,0,0,0,0],shadow=True)\n",
    "# Unsquish the pie.\n",
    "import matplotlib.pyplot as plt\n",
    "plt.gca().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary issues for all the complaints in California are Loan modification,collection,foreclosure,Loan servicing, payments, escrow account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the various product types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_product_discussions = round(df[\"Product\"].value_counts() / len(df[\"Product\"]) * 100,2)\n",
    "\n",
    "print(p_product_discussions)\n",
    "\n",
    "labels = list(p_product_discussions.index)\n",
    "values = p_product_discussions.values.tolist()\n",
    "colors = ['#F78181', '#F5A9BC', '#2E9AFE', '#58FA58', '#FAAC58', '#088A85', '#8A0808', '#848484', '#F781F3', '#D7DF01', '#2E2EFE']\n",
    "\n",
    "\n",
    "product_pie = go.Pie(labels=labels, values=values, \n",
    "                          marker=dict(colors=colors,\n",
    "                         line=dict(color='#000000', width=2)))\n",
    "\n",
    "layout = go.Layout(title='Product Types')\n",
    "\n",
    "fig = go.Figure(data=[product_pie], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the variables for our graphs\n",
    "disputed = df['Consumer disputed?'].value_counts()\n",
    "company_response = df['Company response to consumer'].value_counts()\n",
    "top5_disputed = df['Company'].loc[df['Consumer disputed?'] == 'Yes'].value_counts()[:5]\n",
    "top5_nodispute = df['Company'].loc[df['Consumer disputed?'] == 'No'].value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Top three subplots\n",
    "total_complaints_plotly = go.Bar(\n",
    "            x=disputed.index.values,\n",
    "            y=disputed.values,\n",
    "    text = 'Complaints',\n",
    "    showlegend=False,\n",
    "    marker=dict(\n",
    "        color=['#40FF00', '#FF0000'])\n",
    "    )\n",
    "\n",
    "top5_disputes_plotly = go.Bar(\n",
    "            x=top5_disputed.index.values,\n",
    "            y=top5_disputed.values,\n",
    "    text='Disputes',\n",
    "    showlegend=False,\n",
    "    marker=dict(\n",
    "        color=top5_disputed.values,\n",
    "        colorscale='Reds')\n",
    "    )\n",
    "\n",
    "top5_nodisputes_plotly = go.Bar(\n",
    "            x=top5_nodispute.index.values,\n",
    "            y=top5_nodispute.values,\n",
    "    text='No Disputes',\n",
    "    showlegend=False,\n",
    "    marker=dict(\n",
    "        color=top5_nodispute.values,\n",
    "        colorscale='Blues')\n",
    "    )\n",
    "\n",
    "# Lower Subplot\n",
    "customer_res_plotly = go.Bar(\n",
    "            x=company_response.index.values,\n",
    "            y=company_response.values,\n",
    "    text='Customer Response',\n",
    "    showlegend=False,\n",
    "        marker=dict(\n",
    "        color=df['Company response to consumer'].value_counts().values,\n",
    "        colorscale = [[0.0, 'rgb(165,0,38)'], [0.1111111111111111, 'rgb(215,48,39)'], [0.2222222222222222, 'rgb(244,109,67)'], \n",
    "                      [0.3333333333333333, 'rgb(253,174,97)'], [0.4444444444444444, 'rgb(254,224,144)'], \n",
    "                      [0.5555555555555556, 'rgb(224,243,248)'], [0.6666666666666666, 'rgb(171,217,233)'], \n",
    "                      [0.7777777777777778, 'rgb(116,173,209)'], [0.8888888888888888, 'rgb(69,117,180)'], \n",
    "                      [1.0, 'rgb(49,54,149)']],\n",
    "        reversescale = True\n",
    "        )\n",
    ")\n",
    "\n",
    "fig = tls.make_subplots(rows=2, cols=3, specs=[[{}, {}, {}], [{'colspan': 3}, None, None]],\n",
    "                          subplot_titles=('Did the Customer Disputed?',\n",
    "                                          'Disputes',\n",
    "                                         'No Disputes',\n",
    "                                         'Company response to consumer'))\n",
    "\n",
    "# First three Subplots\n",
    "fig.append_trace(total_complaints_plotly, 1, 1)\n",
    "fig.append_trace(top5_disputes_plotly , 1, 2)\n",
    "fig.append_trace(top5_nodisputes_plotly , 1, 3)\n",
    "\n",
    "# Lower Subplot\n",
    "fig.append_trace(customer_res_plotly, 2, 1)\n",
    "\n",
    "\n",
    "\n",
    "fig['layout'].update(showlegend=True, height=600, width=800, title='Sectors')\n",
    "iplot(fig, filename='Complaints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date received'] = pd.to_datetime(df['Date received'])\n",
    "df['year_received'], df['month_received'] = df['Date received'].dt.year, df['Date received'].dt.month\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Line Plot by the top 5 companies  by year who had more customer interaction cases (Disputes and No Disputes)\n",
    "sorting_groups = df.groupby(['year_received', 'Consumer disputed?'])['Company'].apply(lambda x: x.value_counts())\n",
    "d = {'CRM': sorting_groups}\n",
    "\n",
    "\n",
    "year_crm = pd.DataFrame(data=d).reset_index()\n",
    "year_crm.sort_values(by='CRM', ascending=False)\n",
    "\n",
    "crm_df = year_crm.rename(columns={\"level_2\": \"Company\"})\n",
    "\n",
    "# Conditionals Top 5 Companies with dispues (Bank of America, Wells Fargo, JP Morgan, Equifax, CitiBank)\n",
    "boa_disputes = crm_df.loc[(crm_df['Company'] == 'Bank of America') & (crm_df['Consumer disputed?'] == 'Yes')]\n",
    "wfc_disputes = crm_df.loc[(crm_df['Company'] == 'Wells Fargo & Company') & (crm_df['Consumer disputed?'] == 'Yes')]\n",
    "jp_disputes = crm_df.loc[(crm_df['Company'] == 'JPMorgan Chase & Co.') & (crm_df['Consumer disputed?'] == 'Yes')]\n",
    "equi_disputes = crm_df.loc[(crm_df['Company'] == 'Equifax') & (crm_df['Consumer disputed?'] == 'Yes')]\n",
    "citi_disputes = crm_df.loc[(crm_df['Company'] == 'Citibank') & (crm_df['Consumer disputed?'] == 'Yes')]\n",
    "\n",
    "# Establish the year (Continue Here tomorrow!)\n",
    "years = boa_disputes['year_received'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing line chart (top 5 companies with complaints)\n",
    "boa_disputes_amount = boa_disputes['CRM'].values.tolist()\n",
    "wfc_disputes_amount = wfc_disputes['CRM'].values.tolist()\n",
    "jp_disputes_amount = jp_disputes['CRM'].values.tolist()\n",
    "equi_disputes_amount = equi_disputes['CRM'].values.tolist()\n",
    "citi_disputes_amount = citi_disputes['CRM'].values.tolist()\n",
    "\n",
    "# Text to add\n",
    "boa_text = [str(dis) + '\\n Disputes'  for dis in boa_disputes_amount]\n",
    "wfc_text = [str(wfc) + '\\n Disputes'  for wfc in wfc_disputes_amount]\n",
    "jp_text = [str(jp) + '\\n Disputes' for jp in jp_disputes_amount]\n",
    "equi_text = [str(equi) + '\\n Disputes' for equi in equi_disputes_amount]\n",
    "citi_text = [str(citi) + '\\n Disputes' for citi in citi_disputes_amount]\n",
    "\n",
    "boa_disputes_chart = go.Scatter(\n",
    "    x=years,\n",
    "    y=boa_disputes_amount,\n",
    "    text=boa_text,\n",
    "    name='Bank of America', \n",
    "    hoverinfo='x+text',\n",
    "    mode='lines',\n",
    "    line=dict(width=1,\n",
    "             color='rgb(0, 22, 235)',\n",
    "             ),\n",
    "    fill='tonexty'\n",
    ")\n",
    "\n",
    "wfc_disputes_chart = go.Scatter(\n",
    "    x=years,\n",
    "    y=wfc_disputes_amount,\n",
    "    text=wfc_text,\n",
    "    name=\"Wells Fargo & Company\", \n",
    "    hoverinfo='x+text',\n",
    "    mode='lines',\n",
    "    line=dict(width=1,\n",
    "             color='rgb(275, 170, 0)',\n",
    "             ),\n",
    "    fill='tonexty'\n",
    ")\n",
    "\n",
    "\n",
    "jp_disputes_chart = go.Scatter(\n",
    "    x=years,\n",
    "    y=jp_disputes_amount,\n",
    "    text=jp_text,\n",
    "    name='JP Morgan Chase & Co.',\n",
    "    hoverinfo='x+text',\n",
    "    mode='lines',\n",
    "    line=dict(width=1,\n",
    "             color='rgb(128, 128, 128)',\n",
    "             ),\n",
    "    fill='tonexty'\n",
    ")\n",
    "\n",
    "equi_disputes_chart = go.Scatter(\n",
    "    x=years,\n",
    "    y=equi_disputes_amount,\n",
    "    text=equi_text,\n",
    "    name='Equifax',\n",
    "    hoverinfo='x+text',\n",
    "    mode='lines',\n",
    "    line=dict(width=1,\n",
    "             color='rgb(175, 0, 0)',\n",
    "             ),\n",
    "    fill='tonexty'\n",
    ")\n",
    "\n",
    "citi_disputes_chart = go.Scatter(\n",
    "    x=years,\n",
    "    y=citi_disputes_amount,\n",
    "    text=citi_text,\n",
    "    name='CitiBank',\n",
    "    hoverinfo='x+text',\n",
    "    mode='lines',\n",
    "    line=dict(width=1,\n",
    "             color='rgb(0, 215, 215)',\n",
    "             ),\n",
    "    fill='tonexty'\n",
    ")\n",
    "\n",
    "data = [boa_disputes_chart, wfc_disputes_chart, jp_disputes_chart, equi_disputes_chart, citi_disputes_chart]\n",
    "\n",
    "layout = dict(title = 'Number of Disputes <br> (Top 5 Companies)',\n",
    "              xaxis = dict(title = 'Year'),\n",
    "              yaxis = dict(title = 'Number of Disputes')\n",
    "             )\n",
    "\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "\n",
    "iplot(fig, filename='basic-area-no-bound')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall the number of Complaints have gone down for all the top 5 Companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Months with the highest disputes (We will make a barplot)\n",
    "def customerservice_per_month(month, dispute):\n",
    "    result = df.loc[(df['month_received'] == month) & (df['Consumer disputed?'] == dispute)]\n",
    "    return result\n",
    "\n",
    "# Monthly Disputes\n",
    "dis_january = len(customerservice_per_month(month=1, dispute='Yes'))\n",
    "dis_february = len(customerservice_per_month(month=2, dispute='Yes'))\n",
    "dis_march = len(customerservice_per_month(month=3, dispute='Yes'))\n",
    "dis_april = len(customerservice_per_month(month=4, dispute='Yes'))\n",
    "dis_may = len(customerservice_per_month(month=5, dispute='Yes'))\n",
    "dis_june = len(customerservice_per_month(month=6, dispute='Yes'))\n",
    "dis_july = len(customerservice_per_month(month=7, dispute='Yes'))\n",
    "dis_august = len(customerservice_per_month(month=8, dispute='Yes'))\n",
    "dis_september = len(customerservice_per_month(month=9, dispute='Yes'))\n",
    "dis_october = len(customerservice_per_month(month=10, dispute='Yes'))\n",
    "dis_november = len(customerservice_per_month(month=11, dispute='Yes'))\n",
    "dis_december = len(customerservice_per_month(month=12, dispute='Yes'))\n",
    "\n",
    "# Monthly No-Disputes\n",
    "nodis_january = len(customerservice_per_month(month=1, dispute='No'))\n",
    "nodis_february = len(customerservice_per_month(month=2, dispute='No'))\n",
    "nodis_march = len(customerservice_per_month(month=3, dispute='No'))\n",
    "nodis_april = len(customerservice_per_month(month=4, dispute='No'))\n",
    "nodis_may = len(customerservice_per_month(month=5, dispute='No'))\n",
    "nodis_june = len(customerservice_per_month(month=6, dispute='No'))\n",
    "nodis_july = len(customerservice_per_month(month=7, dispute='No'))\n",
    "nodis_august = len(customerservice_per_month(month=8, dispute='No'))\n",
    "nodis_september = len(customerservice_per_month(month=9, dispute='No'))\n",
    "nodis_october = len(customerservice_per_month(month=10, dispute='No'))\n",
    "nodis_november = len(customerservice_per_month(month=11, dispute='No'))\n",
    "nodis_december = len(customerservice_per_month(month=12, dispute='No'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most active months\n",
    "months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September',\n",
    "         'October', 'November', 'December']\n",
    "disputes_by_month = [dis_january, dis_february, dis_march, dis_april, dis_may, dis_june, dis_july, dis_august, dis_september,\n",
    "                    dis_october, dis_november, dis_december]\n",
    "\n",
    "nodisputes_by_month = [nodis_january, nodis_february, nodis_march, nodis_april, nodis_may, nodis_june, nodis_july, \n",
    "                       nodis_august, nodis_september, nodis_october, nodis_november, nodis_december]\n",
    "\n",
    "\n",
    "disputes_chart = go.Bar(\n",
    "    y=months,\n",
    "    x=disputes_by_month,\n",
    "    orientation='h',\n",
    "    name='Disputes',\n",
    "    text='Disputes',\n",
    "    marker=dict(\n",
    "        color='#FF6464',\n",
    "    line=dict(\n",
    "        color='#CD3232',\n",
    "        width=1.5\n",
    "    ))\n",
    ")\n",
    "\n",
    "nodisputes_chart = go.Bar(\n",
    "    y=months,\n",
    "    x=nodisputes_by_month,\n",
    "    orientation='h',\n",
    "    name='No Disputes',\n",
    "    text='No Disputes',\n",
    "    marker=dict(\n",
    "        color='#A9FFA9',\n",
    "    line=dict(\n",
    "        color='#59AF59',\n",
    "        width=1.5\n",
    "    ))\n",
    ")\n",
    "\n",
    "fig = tls.make_subplots(rows=1, cols=2, specs=[[{}, {}]],\n",
    "                          subplot_titles=('Dispute Chart per Month',\n",
    "                                          'No Dispute Chart per Month'))\n",
    "\n",
    "fig.append_trace(disputes_chart, 1, 1)\n",
    "fig.append_trace(nodisputes_chart, 1, 2)\n",
    "\n",
    "fig['layout'].update(showlegend=True, title=\"Level of Activity by Month\")\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "July receives the most number of disputes during the year while March receives the lowest number of disputes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Company'])['Complaint ID'].count().sort_values(ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispute_presence = df.loc[df['Consumer disputed?'] == 'Yes']\n",
    "cross_month = pd.crosstab(dispute_presence['State'], dispute_presence['Company']).apply(lambda x: x/x.sum() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bank of America has the highest number of complaints among all the other companies. Lets see where are these complains from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Share of Most disputes for Bank of America.\n",
    "df_boa = pd.DataFrame(cross_month['Bank of America']).reset_index().sort_values(by=\"Bank of America\", ascending=False).round(2)\n",
    "df_boa = df_boa.rename(columns={'Bank of America': 'share of complaints'})\n",
    "\n",
    "for col in df_boa.columns:\n",
    "    df_boa[col] = df_boa[col].astype(str)\n",
    "    \n",
    "    \n",
    "scl = [[0.0, 'rgb(202, 202, 202)'],[0.2, 'rgb(253, 205, 200)'],[0.4, 'rgb(252, 169, 161)'],\n",
    "            [0.6, 'rgb(247, 121, 108  )'],[0.8, 'rgb(255, 39, 39)'],[1.0, 'rgb(219, 0, 0)']]\n",
    "\n",
    "\n",
    "df_boa['text'] = \"State Code: \" + df_boa['State'] + '<br>'\n",
    "\n",
    "\n",
    "data = [ dict(\n",
    "        type='choropleth',\n",
    "        colorscale = scl,\n",
    "        autocolorscale = False,\n",
    "        locations = df_boa['State'],\n",
    "        z = df_boa['share of complaints'], \n",
    "        locationmode = 'USA-states',\n",
    "        text = df_boa['text'],\n",
    "        marker = dict(\n",
    "            line = dict (\n",
    "                color = 'rgb(255,255,255)',\n",
    "                width = 2\n",
    "            ) ),\n",
    "        colorbar = dict(\n",
    "            title = \"%\")\n",
    "        ) ]\n",
    "\n",
    "layout = dict(\n",
    "    title = 'Most Complaints by State <br> Bank of America',\n",
    "    geo = dict(\n",
    "        scope = 'usa',\n",
    "        projection=dict(type='albers usa'),\n",
    "        showlakes = True,\n",
    "        lakecolor = 'rgb(255, 255, 255)')\n",
    ")\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "iplot(fig, filename='d3-cloropleth-map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Company'] == 'Bank of America']['Product'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, most of the complaints that Bank of America receives are regarding Mortgages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['Timely response?'] == 'Yes'])/len(df['Timely response?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the Timely response rate is quite high. Let's how it impacts the Consumer Disputes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['Timely response?'] == 'Yes') & (df['Consumer disputed?'] == 'Yes')]['Company'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[(df['Timely response?'] == 'Yes') & (df['Consumer disputed?'] == 'Yes')])/len(df[df['Consumer disputed?'] == 'Yes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for most of the companies the Consumer disputed after the timely response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['Timely response?'] == 'No') & (df['Consumer disputed?'] == 'Yes')]['Company'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[(df['Timely response?'] == 'No') & (df['Consumer disputed?'] == 'Yes')])/len(df[df['Consumer disputed?'] == 'Yes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of disputes after a delay in response in really low. However, this does not signify that late responses will not attract dispute. It may mean that the consumers issue is either resolved or he is no longer interested in the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Complaints_by_products = df.groupby(['Product'])['Complaint ID'].count().sort_values(ascending=False)\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "Complaints_by_products.plot.barh(ylim=0,color='blue',title= 'Customer Complaints By Products\\n')\n",
    "plt.xlabel('No of Complaints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Product'] == 'Mortgage']['Issue'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most of the complaints for Mortgage were regarding Loan modification,collection,foreclosure,Loan servicing, payments, escrow account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitted_via_df = df['Submitted via'].value_counts()\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "submitted_via_df.plot.pie(title= 'How the Customers Complaints were submitted\\n',explode=[0.2,0,0,0,0,0],shadow=True)\n",
    "plt.xlabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than half of the Users submitted their complaints via web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Issue_df = df['Issue'].value_counts().head(10)\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "Issue_df.plot.barh(ylim=0, color='blue', title= 'Consumer Complaint Issues\\n')\n",
    "plt.xlabel('No of Complaints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand how the No of Complaints changed over the years and months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : TEXT BASED MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains features that are not necessary to solve our multi-classification problem. For this text classification problem, we are going to build another dataframe that contains ‘Product’ and ‘Consumer complaint narrative’ (renamed as 'Consumer_complaint')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe with two columns\n",
    "df1 = df[['Product', 'Consumer complaint narrative']].copy()\n",
    "\n",
    "# Remove missing values (NaN)\n",
    "df1 = df1[pd.notnull(df1['Consumer complaint narrative'])]\n",
    "\n",
    "# Renaming second column for a simpler name\n",
    "df1.columns = ['Product', 'Consumer_complaint'] \n",
    "\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of complaints with text\n",
    "total = df1['Consumer_complaint'].notnull().sum()\n",
    "round((total/len(df)*100),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From more than 358810 complaints, there are about 56180 cases with text (~ 15.7% of the original dataset is not null). This is still a good number to work with. Now let's have a look at the categories we want to classify each complaint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.Product.unique()).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 12 different classes or categories (target). However; it is observed that some classes are contained in others. For instance, ‘Credit card’ and ‘Prepaid card’ are contained in ‘Credit card or prepaid card’ category. Now, imagine there is a new complaint about Credit card and we want to classify it. The algorithm can either classify this complaint as 'Credit card' or 'Credit card or prepaid' and it would be correct. Nevertheless, this would affect model performance. In order to avoid this problem, the names of some categories were renamed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the computation is time consuming (in terms of CPU), the data was sampled\n",
    "df2 = df1.sample(10000, random_state=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming categories\n",
    "df2.replace({'Product': \n",
    "             {'Credit reporting, credit repair services, or other personal consumer reports': \n",
    "              'Credit reporting, repair, or other', \n",
    "              'Credit reporting': 'Credit reporting, repair, or other',\n",
    "             'Credit card': 'Credit card or prepaid card',\n",
    "             'Prepaid card': 'Credit card or prepaid card',\n",
    "             'Payday loan': 'Payday loan, title loan, or personal loan',\n",
    "             'Money transfer': 'Money transfer, virtual currency, or money service',\n",
    "             'Virtual currency': 'Money transfer, virtual currency, or money service'}}, \n",
    "            inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df2.Product.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of classes were reduced from 12 to 10. <br><br>Now we need to represent each class as a number, so as our predictive model can better understand the different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'category_id' with encoded categories \n",
    "df2['category_id'] = df2['Product'].factorize()[0]\n",
    "category_id_df = df2[['Product', 'category_id']].drop_duplicates()\n",
    "\n",
    "\n",
    "# Dictionaries for future use\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'Product']].values)\n",
    "\n",
    "# New dataframe\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "colors = ['grey','grey','grey','grey','grey','grey','grey','grey','grey',\n",
    "    'grey','darkblue','darkblue','darkblue']\n",
    "df2.groupby('Product').Consumer_complaint.count().sort_values().plot.barh(\n",
    "    ylim=0, color='Blue', title= 'NUMBER OF COMPLAINTS IN EACH PRODUCT CATEGORY\\n')\n",
    "plt.xlabel('Number of ocurrences', fontsize = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pre'></a>\n",
    "## Text Preprocessing :\n",
    "\n",
    "The text needs to be transformed to vectors so as the algorithms will be able make predictions. In this case it will be used the Term Frequency – Inverse Document Frequency (TFIDF) weight to evaluate __how important a word is to a document in a collection of documents__.\n",
    "\n",
    "After removing __punctuation__ and __lower casing__ the words, importance of a word is determined in terms of its frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### “Term Frequency – Inverse Document Frequency \n",
    "\n",
    "__TF-IDF__ is the product of the __TF__ and __IDF__ scores of the term.<br><br> $$\\text{TF-IDF}=\\frac{\\text{TF}}{\\text{IDF}}$$<br>\n",
    "\n",
    "__Term Frequency :__ This summarizes how often a given word appears within a document.\n",
    "\n",
    "$$\\text{TF} = \\frac{\\text{Number of times the term appears in the doc}}{\\text{Total number of words in the doc}}$$<br><br>\n",
    "__Inverse Document Frequency:__ This downscales words that appear a lot across documents. A term has a high IDF score if it appears in a few documents. Conversely, if the term is very common among documents (i.e., “the”, “a”, “is”), the term would have a low IDF score.<br>\n",
    "\n",
    "$$\\text{IDF} = \\ln\\left(\\frac{\\text{Number of docs}}{\\text{Number docs the term appears in}} \\right)$$<br>\n",
    "\n",
    "TF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents. The higher the TFIDF score, the rarer the term is. For instance, in a Mortgage complaint the word _mortgage_ would be mentioned fairly often. However, if we look at other complaints, _mortgage_ probably would not show up in many of them. We can infer that _mortgage_ is most probably an important word in Mortgage complaints as compared to the other products. Therefore, _mortgage_ would have a high TF-IDF score for Mortgage complaints.\n",
    "\n",
    "TfidfVectorizer class can be initialized with the following parameters:\n",
    "* __min_df__: remove the words from the vocabulary which have occurred in less than ‘min_df’ number of files.\n",
    "* __max_df__: remove the words from the vocabulary which have occurred in more than _‘max_df’ * total number of files in corpus_.\n",
    "* __sublinear_tf__: set to True to scale the term frequency in logarithmic scale.\n",
    "* __stop_words__: remove the predefined stop words in 'english'.\n",
    "* __use_idf__: weight factor must use inverse document frequency.\n",
    "* __ngram_range__: (1, 2) to indicate that unigrams and bigrams will be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                        ngram_range=(1, 2), \n",
    "                        stop_words='english')\n",
    "\n",
    "# We transform each complaint into a vector\n",
    "features = tfidf.fit_transform(df2.Consumer_complaint).toarray()\n",
    "\n",
    "labels = df2.category_id\n",
    "\n",
    "print(\"Each of the %d complaints is represented by %d features (TF-IDF score of unigrams and bigrams)\" %(features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the three most correlated terms with each of the product categories\n",
    "N = 3\n",
    "for Product, category_id in sorted(category_to_id.items()):\n",
    "  features_chi2 = chi2(features, labels == category_id)\n",
    "  indices = np.argsort(features_chi2[0])\n",
    "  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "  print(\"\\n==> %s:\" %(Product))\n",
    "  print(\"  * Most Correlated Unigrams are: %s\" %(', '.join(unigrams[-N:])))\n",
    "  print(\"  * Most Correlated Bigrams are: %s\" %(', '.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ml'></a>\n",
    "## Multi-Classification models : \n",
    "\n",
    "The classification models evaluated are: \n",
    "* Random Forest\n",
    "* Linear Support Vector Machine\n",
    "* Multinomial Naive Bayes \n",
    "* Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sp'></a>\n",
    "### Spliting the data into train and test sets\n",
    "The original data was divided into features (X) and target (y), which were then splitted into train (75%) and test (25%) sets. Thus, the algorithms would be trained on one set of data and tested out on a completely different set of data (not seen before by the algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2['Consumer_complaint'] # Collection of documents\n",
    "y = df2['Product'] # Target or the labels we want to predict (i.e., the 13 different complaints of products)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m'></a>\n",
    "## Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]\n",
    "\n",
    "# 5 Cross-validation\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "\n",
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "    \n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sum'></a>\n",
    "## Comparison of model performance\n",
    "\n",
    "The best mean acuracy was obtained with LinearSVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy = cv_df.groupby('model_name').accuracy.mean()\n",
    "std_accuracy = cv_df.groupby('model_name').accuracy.std()\n",
    "\n",
    "acc = pd.concat([mean_accuracy, std_accuracy], axis= 1, \n",
    "          ignore_index=True)\n",
    "acc.columns = ['Mean Accuracy', 'Standard deviation']\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(x='model_name', y='accuracy', \n",
    "            data=cv_df, \n",
    "            color='lightblue', \n",
    "            showmeans=True)\n",
    "plt.title(\"MEAN ACCURACY (cv = 5)\\n\", size=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ev'></a>\n",
    "## Model Evaluation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test,indices_train,indices_test = train_test_split(features, \n",
    "                                                               labels, \n",
    "                                                               df2.index, test_size=0.25, \n",
    "                                                               random_state=1)\n",
    "model = LinearSVC()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['category_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='m'></a>\n",
    "## Precision, Recall, F1-score :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t\\t\\t\\tCLASSIFICATIION METRICS\\n')\n",
    "print(metrics.classification_report(y_test, y_pred, \n",
    "                                    target_names= df2['Product'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cm'></a>\n",
    "## Confusion Matrix : \n",
    "\n",
    "A Confusion Matrix is a table which rows represent the actual class and columns represents the predicted class.<br><br>\n",
    "If we had a perfect model that always classifies correctly a new complaint, then the confusion matrix would have values in the diagonal only (where predicted label = actual label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "sns.heatmap(conf_mat, annot=True, cmap=\"Blues\", fmt='d',\n",
    "            xticklabels=category_id_df.Product.values, \n",
    "            yticklabels=category_id_df.Product.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title(\"CONFUSION MATRIX - LinearSVC\\n\", size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the confusion matrix looks good (clear diagonal that represents correct classifications). Nevertheless, there are cases were the complaint was classified in a wrong class.\n",
    "\n",
    "## Misclassified complaints :\n",
    "Let’s have a look at the cases that were wrongly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for predicted in category_id_df.category_id:\n",
    "  for actual in category_id_df.category_id:\n",
    "    if predicted != actual and conf_mat[actual, predicted] >= 20:\n",
    "      print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual], \n",
    "                                                           id_to_category[predicted], \n",
    "                                                           conf_mat[actual, predicted]))\n",
    "    \n",
    "      display(df2.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['Product', \n",
    "                                                                'Consumer_complaint']])\n",
    "      print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most correlated terms with each category : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(features, labels)\n",
    "\n",
    "N = 4\n",
    "for Product, category_id in sorted(category_to_id.items()):\n",
    "  indices = np.argsort(model.coef_[category_id])\n",
    "  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "  unigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 1][:N]\n",
    "  bigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 2][:N]\n",
    "  print(\"\\n==> '{}':\".format(Product))\n",
    "  print(\"  * Top unigrams: %s\" %(', '.join(unigrams)))\n",
    "  print(\"  * Top bigrams: %s\" %(', '.join(bigrams)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pred'></a>\n",
    "# Predictions :\n",
    "\n",
    "Now let's make a few predictions on unseen data.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state = 0)\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                        ngram_range=(1, 2), \n",
    "                        stop_words='english')\n",
    "\n",
    "fitted_vectorizer = tfidf.fit(X_train)\n",
    "tfidf_vectorizer_vectors = fitted_vectorizer.transform(X_train)\n",
    "\n",
    "model = LinearSVC().fit(tfidf_vectorizer_vectors, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what is the classification that our model gives to this new complaint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_complaint = \"\"\"I have been enrolled back at XXXX XXXX University in the XX/XX/XXXX. Recently, i have been harassed by \\\n",
    "Navient for the last month. I have faxed in paperwork providing them with everything they needed. And yet I am still getting \\\n",
    "phone calls for payments. Furthermore, Navient is now reporting to the credit bureaus that I am late. At this point, \\\n",
    "Navient needs to get their act together to avoid me taking further action. I have been enrolled the entire time and my \\\n",
    "deferment should be valid with my planned graduation date being the XX/XX/XXXX.\"\"\"\n",
    "print(model.predict(fitted_vectorizer.transform([new_complaint])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm has classified this text as a \"Student loan\" complaint. The complaint is regarding Student Loans and it has predicted it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_complaint_2 = \"\"\"I have been getting robo calls from a debt collection agency called \" Alliance 1 \\'\\' for over XXXX months. \n",
    "The calls average XXXX times per week. They are attempting to collect a debt for someone whose name sounds \n",
    "like \" XXXX XXXX \\'\\'. I am sick and tired of their harrassement and want the calls to stop. \\n\"\"\"\n",
    "print(model.predict(fitted_vectorizer.transform([new_complaint_2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the algorithm correctly classified the caomplaint as __\"Debt collection\"__. \n",
    "Although our model is not going to be all the time correct when classifying new complaints, it does a good job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../input/consumer-complaints-test-dataset/Edureka_Consumer_Complaints_test.csv')\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with two columns\n",
    "df3 = test_df[['Product', 'Consumer complaint narrative']].copy()\n",
    "\n",
    "# Remove missing values (NaN)\n",
    "df3 = df3[pd.notnull(df3['Consumer complaint narrative'])]\n",
    "\n",
    "# Renaming second column for a simpler name\n",
    "df3.columns = ['Product', 'Consumer_complaint'] \n",
    "\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of complaints with text\n",
    "total = df3['Consumer_complaint'].notnull().sum()\n",
    "round((total/len(test_df)*100),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df3.Product.unique()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df2.Product.unique()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming categories\n",
    "df3.replace({'Product': \n",
    "             {'Credit reporting, credit repair services, or other personal consumer reports': \n",
    "              'Credit reporting, repair, or other', \n",
    "              'Credit reporting': 'Credit reporting, repair, or other',\n",
    "             'Credit card': 'Credit card or prepaid card',\n",
    "             'Prepaid card': 'Credit card or prepaid card',\n",
    "             'Payday loan': 'Payday loan, title loan, or personal loan',\n",
    "             'Money transfer': 'Money transfer, virtual currency, or money service',\n",
    "             'Virtual currency': 'Money transfer, virtual currency, or money service'}}, \n",
    "            inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df2.Product.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions = model.predict(fitted_vectorizer.transform(df3['Consumer_complaint']))\n",
    "Predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of the Predictions is same as the test date i.e no of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions_df = pd.DataFrame(Predictions,columns=['Predictions']).reset_index()\n",
    "Predictions_df.to_csv('predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transferring the Predictions to predictions.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t\\t\\t\\tCLASSIFICATIION METRICS\\n')\n",
    "print(metrics.classification_report(df3['Product'], Predictions, \n",
    "                                    target_names= df3['Product'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is predicting most of categories correctly with an accuracy of around 0.80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(df3['Product'], Predictions)\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "sns.heatmap(conf_mat, annot=True, cmap=\"Blues\", fmt='d',\n",
    "            xticklabels=category_id_df.Product.values, \n",
    "            yticklabels=category_id_df.Product.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title(\"CONFUSION MATRIX - LinearSVC\\n\", size=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Consumer_complaint'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[df3['Consumer_complaint'] == df3['Consumer_complaint'].iloc[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaint1 = \"\"\"I WANT TO REQUEST A CREDIT LINE INCREASE OF XXXX I FELL I HAVE THE RIGHT CREDIT LINE DO TO MY INCOME AND THE LADY FROM CAPITAL ONE SAID IT IS BASED OFF INCOME \n",
    "AND ALSO DUE TO THE FACT THEY HAVE NOT MAILED MY CREDIT CARD TO ME YET AFTER I CALLED AND TOLD THEM TO. \\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(fitted_vectorizer.transform([complaint1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has predicted the first complaint correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[df3['Consumer_complaint'] == df3['Consumer_complaint'].iloc[16]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Consumer_complaint'].iloc[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaint2 = \"\"\"I have been actively filing loan modifications with B of A sinceXXXX. I am a XXXX Veteran and I did use my VA certificateto secure my home in XXXX. My home was finance thruCountry Wide before B of A bought my loan. B of A didservice my home for foreclosure while I was serving in theXXXX XXXX about XXXX XXXX and the loan was noteven 2 months behind. I came home in XXXX XXXX andrequested another loan modification but no response soI obtained an attorney. My attorney did respond to B ofA and requested all correspondence concerning my homebe sent to them and they were actively involved with myloan modification. In early XXXX I was notified by areal-estate agent that my home was listed for foreclosureand public auction. No XXXX service me or my attorney. Theforeclosure was listed in the local newspaper. This wasnews to me and my attorney since we both had no ideaand was actively involve with a loan modification at thetime. I have since been working with Attorney Generaloffice in Florida. I was given guidance from the AG tofile this complaint with CFPB along with their inquireabout my case. I have been told my request formodification has been denied for reasons such as \\'\\' I was not living in the home and it is a VA require-ment \\'\\' which is not true, \" insufficient income \\'\\', mycurrent husband signed a form stating he wouldcontribute {$1500.00} additional income to my household to help cover the cost and the list goes on. \\nI am currently enrolled in a XXXX programwith the XXXX VA and is receivingtreatment. My condition is described as \" XXXX \\'\\' at this time but with support from my husbandand adult children I am trying to do what isnecessary to keep my home. I am filing thiscomplaint because I too believe I am a victimof \" dual tracking \\'\\' with B of A. They are notowning up to their constant mistakes. I don\\'tunderstand why they will not work with me tohelp me keep my home vice foreclosing. I amnot trying to run from my responsibility of payingfor my home. I just need help with lowering the payment so I can afford it. Is that too much to ask for? \\nB of A pulled back the foreclosure because they justrealized we were working on a loan modification with mebut recently filed a petition in court another foreclosure. \\nI have no other option but to file bankruptcy to keepmy home. This not fair to me the consumer because allI wanted was a lower payment. \\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(fitted_vectorizer.transform([complaint2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has predicted the second complaint correctly as well. Thus, the model is doing a good job in predicting the product category using the Consumer Complaint narratives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 : CLASSIFICATION MODELS AND FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/consumer-complaints-train-dataset/Edureka_Consumer_Complaints_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a copy of the dataset to perform the operations and prepare a good dataset for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop(['Date received','Date sent to company','Sub-product','Issue', \n",
    "          'Sub-issue','Consumer complaint narrative','ZIP code','Complaint ID'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping Sub-product,Issue, Sub-issue as all of them are related to the Product Column. Dropping the Date columns as they don't have much contribution and the same goes for ZIP code and Complaint ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there a lot of null values in the Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Company public response'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common company response is Company chooses not to provide a public response which we can use to fill the remaining NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Consumer consent provided?'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Consumer consent provided? column Other seems to be a neutral column that can be used to fill in the NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Tags'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information available is very less and there is no neutral value that we can use to fill the NA values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop(['Tags'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the Tags column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Company public response'].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Consumer consent provided?'].fillna('Other',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling the NA values in **Consumer consent provided?** with **'Other'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Consumer consent provided?'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Company public response'].fillna('Company chooses not to provide a public response',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling the NA values in Company public response column with **'Company chooses not to provide a public response'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are NA values in the state columns and it is something we can't just fill in randomly. Hence, dropping those NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **355907** after cleaning which a really good number to work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting all the object columns using ****Label Encoder()****."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the dataset after conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the computation is time consuming (in terms of CPU), the data was sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.sample(10000, random_state=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Consumer disputed?'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that the data isn't balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's define our **X** and **Y** and start working on the training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2.drop(['Consumer disputed?'],axis=1)\n",
    "y = df2['Consumer disputed?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is an **imbalanced dataset**, we will set the <b>class_weight</b> parameter to 'balanced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(random_state=101,class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "print(metrics.accuracy_score(y_test,predictions))\n",
    "print('********************************************')\n",
    "print('Confusion matrix')\n",
    "lr_cfm=metrics.confusion_matrix(y_test, predictions)\n",
    "\n",
    "\n",
    "lbl1=[\"Predicted 1\", \"Predicted 2\"]\n",
    "lbl2=[\"Actual 1\", \"Actual 2\"]\n",
    "\n",
    "sns.heatmap(lr_cfm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=lbl1, yticklabels=lbl2)\n",
    "plt.show()\n",
    "\n",
    "print('**********************************************')\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall accuracy is less and the precision and f1-score is really low for 1. Let's try out other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,predictions)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print (roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy can be low due to the fact that there are a lot of <b>categorical variables</b> in the dataset. As there is a lot of <b>non-linearity</b>, let us use <b>tree</b> algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try the <b>Bagging</b> technique <b>Random Forest</b> on the dataset. Since we do not know the optimal hyperparameters for the forest, let us use GridSearch cross-validation to identify them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search :\n",
    "\n",
    "Here are the parameters for <a href='https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV'>GridSearchCV</a>:\n",
    "- __estimator__: `model`\n",
    "- __param_grid__: `dist or list of dictionaries` Parameters to the estimator/model\n",
    "- __scoring__: `string,callable, list/tuple, dict or None, default: None` Evaluating metrics\n",
    "- __cv__: `int or callable` Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_grid = {'n_estimators': range(5,20,3),\n",
    "              'max_features' : ['auto', 'sqrt'],\n",
    "              'max_depth' : [5,10,20,30],\n",
    "              'min_samples_split':[2,5,10],\n",
    "              'criterion':['entropy'],\n",
    "              'min_samples_leaf':[1,2,4]}\n",
    "\n",
    "rf=RandomForestClassifier(oob_score=True,class_weight='balanced')\n",
    "rf_gs = GridSearchCV(rf, random_grid, cv = 5, n_jobs=-1, verbose=2)\n",
    "\n",
    "rf_gs.fit(X_train, y_train)\n",
    "y_pred = rf_gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(y_test,y_pred))\n",
    "print('*******************************************')\n",
    "print('Confusion matrix')\n",
    "rf_cfm=metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "lbl1=[\"Predicted 1\", \"Predicted 2\"]\n",
    "lbl2=[\"Actual 1\", \"Actual 2\"]\n",
    "\n",
    "sns.heatmap(rf_cfm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=lbl1, yticklabels=lbl2)\n",
    "plt.show()\n",
    "\n",
    "print('********************************************')\n",
    "print(metrics.classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy has surely increased ! But hold on...we need to remember that it is an imbalanced dataset. Let us calculate the null accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Values of y_test\n",
    "print (y_test.value_counts())\n",
    "print (\"Null Accuracy:\",y_test.value_counts().head(1) / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null accuracy itself is **0.79**. So our accuracy of **0.72** is less than that. Also, as discussed earlier, **f1-score** is more important in these scenarios. But even that is pretty low. By the way, is f1-score still the right metric in this scenario?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For imbalanced datasets, ROC_AUC is considered to be a more relevant metric than f1-score and accuracy as it is independent of threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_probabilities = rf_gs.predict_proba(X_test)\n",
    "final_metric = roc_auc_score(y_test, predict_probabilities[:,1])\n",
    "print (final_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this is a metric that makes sense. We should get the **ROC_AUC** score as close to 1 as possible. Let us now see how Boosting algorithms fare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(num_class = 2,\n",
    "                           objective=\"multi:softprob\",\n",
    "                           eval_metric=\"mlogloss\",\n",
    "                           seed=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_pred=xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(y_test,xgboost_pred))\n",
    "print('************')\n",
    "print('Confusion matrix')\n",
    "xgboost_cm=metrics.confusion_matrix(y_test, xgboost_pred)\n",
    "\n",
    "\n",
    "lbl1=[\"Predicted 1\", \"Predicted 2\"]\n",
    "lbl2=[\"Actual 1\", \"Actual 2\"]\n",
    "\n",
    "sns.heatmap(xgboost_cm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=lbl1, yticklabels=lbl2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('************\\n Classification report')\n",
    "print(metrics.classification_report(y_test,xgboost_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are at a pretty good accuracy of 0.80 which is slightly higher than our Null Accuracy of 0.79 though the recall and f1-score of 1 is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_probabilities = xgb_model.predict_proba(X_test)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "final_metric = roc_auc_score(y_test, predict_probabilities[:,1])\n",
    "print (final_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurray! <b>Xgboost</b> gave us a higher ROC_AUC score compared to Random Forest. Let us see if we can make this better by tuning the real strengths of Xgboost - its <b>Hyperparameters.</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XgBoost with Grid Search :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_gs_clf = XGBClassifier(num_class = 2,\n",
    "                           objective=\"multi:softprob\",\n",
    "                           eval_metric=\"mlogloss\",\n",
    "                           seed=42)                         \n",
    "                        \n",
    "param_grid = {\"max_depth\": [10,15,20],\n",
    "              \"n_estimators\": range(5,20,5) , \n",
    "              \"gamma\": [0.03,0.05], \n",
    "              \"learning_rate\": [0.01,0.05]}\n",
    "#              \"min_child_weight\": [5,10], \n",
    "#              \"colsample_bytree\": [0.4,0.8], \n",
    "#              \"subsample\": [0.50,0.85]} \n",
    "\n",
    "grid_search = GridSearchCV(xgb_gs_clf, \n",
    "                           param_grid=param_grid,\n",
    "                           cv = 5,\n",
    "                           n_jobs=-1,\n",
    "                           scoring='neg_log_loss',\n",
    "                           verbose=2)\n",
    "grid_search.fit(X_train,y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_y_pred=grid_search.predict(X_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(y_test,xgboost_y_pred))\n",
    "print('*************************************************')\n",
    "print('Confusion matrix')\n",
    "xgboost_cfm=metrics.confusion_matrix(y_test, xgboost_y_pred)\n",
    "\n",
    "\n",
    "lbl1=[\"Predicted 1\", \"Predicted 2\"]\n",
    "lbl2=[\"Actual 1\", \"Actual 2\"]\n",
    "\n",
    "sns.heatmap(xgboost_cfm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=lbl1, yticklabels=lbl2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('***************************************************\\n Classification report')\n",
    "print(metrics.classification_report(y_test,xgboost_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_probabilities = grid_search.predict_proba(X_test)\n",
    "final_metric = roc_auc_score(y_test, predict_probabilities[:,1])\n",
    "print (final_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no improvement over the earlier Xgboost model. Let's try out with <b>more hyperparameter combinations</b> and see if there is a jump in the roc_auc value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "d_train= lgb.Dataset(X_train, label = y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['learning_rate'] = 0.001                                             # Learning rate\n",
    "params['boosting_type'] = 'gbdt'                                           # gbdt = gradient boosted decision tree\n",
    "params['objective'] = 'multiclass'                                         # Multi class classification\n",
    "params['metric'] = 'multi_logloss'\n",
    "params['num_classes'] = 5 \n",
    "params['eval_metric']='auc', 'binary_logloss'\n",
    "#params['sub_feature'] = 0.5\n",
    "#params['min_data'] = 50\n",
    "#params['max_depth'] = 10\n",
    "\n",
    "\n",
    "clf = lgb.train(params, d_train)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "predictions_proba = []\n",
    "for x in y_pred:\n",
    "    predictions.append(np.argmax(x))\n",
    "    predictions_proba.append(max(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_y_pred = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(y_test,lgb_y_pred))\n",
    "print('*************************************************')\n",
    "print('Confusion matrix')\n",
    "lgb_cfm=metrics.confusion_matrix(y_test, lgb_y_pred)\n",
    "\n",
    "\n",
    "lbl1=[\"Predicted 1\", \"Predicted 2\"]\n",
    "lbl2=[\"Actual 1\", \"Actual 2\"]\n",
    "\n",
    "sns.heatmap(lgb_cfm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=lbl1, yticklabels=lbl2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('***************************************************\\n Classification report')\n",
    "print(metrics.classification_report(y_test,lgb_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_metric = roc_auc_score(y_test, predictions_proba)\n",
    "print (final_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>LightGBM</b> performance is not upto the mark.It is unable to correctly classify even a single datapoint that belonged to class 1. We can conclude that <b>Consumer Complaints</b> can use the <b>XGboost</b> model to identify if the Consumer Disputed or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out our best model to fit our Test Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../input/consumer-complaints-test-dataset/Edureka_Consumer_Complaints_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean the data in test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.drop(['Date received','Date sent to company','Sub-product','Issue', \n",
    "          'Sub-issue','Consumer complaint narrative','ZIP code','Complaint ID'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.drop(['Tags'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Consumer consent provided?'].fillna('Other',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Company public response'].fillna('Company chooses not to provide a public response',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 118670 after the Data Cleaning, which is a good number to fit the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df1 = test_df.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = xgb_model.predict(test_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transfer our predictions to an external csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_df = pd.DataFrame(test_predictions,columns=['Consumer Disputed Predictions'])\n",
    "test_predictions_df.to_csv('Cust_Dispute_predictions.csv',\"w\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
